---
author: CC-2018
layout: post
title: "spark开发环境"
date: 2017-10-26 10:18:01 +0800
categories: reading
tag: 总结
---

本文记录在单机mac上面，搭建基于hadoop hdfs文件系统，idea集成scala语言开发的spark开发环境

### JDK
jdk安装和配置就不介绍了

### hadoop安装

因为要使用hadoop的hdfs，需要先安装hdoop，这里选择hadoop编译好的版本, [2.8.2](http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.8.2/hadoop-2.8.2.tar.gz)

假设解压到/home/cc/hadoop-2.8.2

切换到解压目录 `cd /home/cc/hadoop-2.8.2`

**环境变量**

可在以下任意一个bash配置文件：
/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc

添加如下配置
```
HADOOP_HOME=/home/cc/hadoop-2.8.2
PATH=$HADOOP_HOME/bin:$PATH
export HADOOP_HOME
export PATH
```
source对应修改的bash文件，将环境变量生效，如：

source /etc/profile

可通过命令`hadoop version` 查看是否设置成功

**配置**

编辑/home/cc/hadoop-2.8.2/etc/hadoop/core-site.xml，配置hadoop持久化目录

```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/cc/hadoop-2.8.2/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9100</value>
    </property>
</configuration>
```

编辑/home/cc/hadoop-2.8.2/etc/hadoop/hdfs-site.xml, 配置副本个数（默认为3，这里配置为1节约空间）, 配置namenode和datanode路径

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/cc/hadoop-2.8.2/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/cc/hadoop-2.8.2/tmp/dfs/data</value>
    </property>
</configuration>
```

执行格式化`hadoop namenode -format`, 生成新的hdfs。

**启动hdfs**

通过bash启动 `sbin/start-dfs.h`, 使用jps命令 可以查看到，多了3个进程：

```
NameNode
DataNode
SecondaryNameNode
```

也可以通过`http://localhost:50070` 查看hdfs状态。

启动成功后就可以通过hadoop命令操作hdfs了。

```
hadoop fs -mkdir /test
hadoop fs -put ./word.log /test
hadoop fs -ls /test
```

### scala安装

scala的版本选择也很重要，看当前下载的spark是基于那个scala版本编译的，就选那个scala版本，否则使用idea集成开发时会遇到很奇怪的问题，如：SparkConf$DeprecatedConfig，NoClassDefFoundError错误。

这里选择spark 2.2.0的编译对应版本 [scala 2.11.11](https://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz)

### spark安装
下载spark编译好的版本 [ 2.2.0 Pre-built for Apache Hadoop 2.7 and later ](http://mirror.bit.edu.cn/apache/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz)

假设解压到/home/cc/spark-2.2.0-bin-hadoop2.7

**环境变量**

可在以下任意一个bash配置文件：
/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc

添加如下配置
```
SPARK_HOME=/home/cc/spark-2.2.0-bin-hadoop2.7
PATH=$SPARK_HOME/bin:$PATH
export SPARK_HOME
export PATH
```
source对应修改的bash文件，将环境变量生效，如：

source /etc/profile

可通过命令 `spark-shell` 进入spark命令行，运行scala测试：

```
val lines = sc.textFile("/tmp/hellospark")
lines.count()
```

如果hdfs已经启动，这里会从hdfs加载文件

以下我们继续配置spark的集群启动

**配置**

拷贝spark-env.sh.template，生成spark-env.sh配置文件，编辑spark-env.sh：

```
export SCALA_HOME=/home/cc/scala-2.11.11
export HADOOP_HOME=/home/cc/hadoop-2.8.2
export HADOOP_CONF_DIR=/home/cc/hadoop-2.8.2/etc/hadoop
#export SPARK_MASTER_IP=localhost
#export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8180
export SPARK_WORKER_MEMORY=1g
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
```

设置hadoop和scala的环境变量，如果曾经设置过，这里可以不用设置；设置hadoop配置文件目录；其余为可选设置，包括：

master ip，端口，webui的端口，限制work进程的cpu核心数，内存使用多大等。

编辑slaves.template，设置slave节点机器：

```
localhost
```

**集群启动**

执行`sbin/start-all.sh`, 启动spark集群。可以看到多了 Master 和 Worker 进程。也可以通过webui监控页面查看：

`http://localhost:8180/` #因为spark配置里设置了webui端口好为8180，不然默认是8080。监控页面里会显示以下重要信息：

```
URL：master url，提交spark作业时要用到这个url，提交作业时不可用localhost代替显示的host名称
REST URL:
workers信息
以及目前运行的作业和完成的作业信息
```

可以运行一个spark的sample：

```
cd /home/cc/spark-2.2.0-bin-hadoop2.7/bin
spark-submit --master spark://MacBook-Pro.local:7077  --class org.apache.spark.examples.SparkPi ../examples/jars/spark-examples_2.11-2.2.0.jar
```

---
author: CC-2018
layout: post
title: "spark开发环境"
date: 2017-10-26 10:18:01 +0800
categories: reading
tag: 总结
---

* content
{:toc}

本文记录在单机mac上面，搭建基于hadoop hdfs文件系统，idea集成scala语言开发的spark开发环境

### JDK

下载安装jdk，并配置环境变量，jdk版本选择1.7以上

### hadoop安装

因为要使用hadoop的hdfs，需要先安装hdoop，这里选择hadoop编译好的版本, [2.8.2](http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.8.2/hadoop-2.8.2.tar.gz)

假设解压到/home/cc/hadoop-2.8.2

切换到解压目录 `cd /home/cc/hadoop-2.8.2`

**设置环境变量**

可在以下任意一个bash配置文件：
/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc

添加如下配置
```
HADOOP_HOME=/home/cc/hadoop-2.8.2
PATH=$HADOOP_HOME/bin:$PATH
export HADOOP_HOME
export PATH
```
source对应修改的bash文件，将环境变量生效，如：

source /etc/profile

可通过命令`hadoop version` 查看是否设置成功

**配置**

编辑/home/cc/hadoop-2.8.2/etc/hadoop/core-site.xml，配置hadoop持久化目录

```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/cc/hadoop-2.8.2/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9100</value>
    </property>
</configuration>
```

编辑/home/cc/hadoop-2.8.2/etc/hadoop/hdfs-site.xml, 配置副本个数（默认为3，这里配置为1节约空间）, 配置namenode和datanode路径

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/cc/hadoop-2.8.2/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/cc/hadoop-2.8.2/tmp/dfs/data</value>
    </property>
</configuration>
```

执行格式化`hadoop namenode -format`, 生成新的hdfs。

**启动hdfs**

通过bash启动 `sbin/start-dfs.h`, 使用jps命令 可以查看到，多了3个进程：

```
NameNode
DataNode
SecondaryNameNode
```

也可以通过`http://localhost:50070` 查看hdfs状态。

启动成功后就可以通过hadoop命令操作hdfs了。

```
hadoop fs -mkdir /test
hadoop fs -put ./word.log /test
hadoop fs -ls /test
```

### scala安装

scala的版本选择也很重要，看当前下载的spark是基于那个scala版本编译的，就选那个scala版本，否则使用idea集成开发时会遇到很奇怪的问题，如：SparkConf$DeprecatedConfig，NoClassDefFoundError错误。

这里选择spark 2.2.0的编译对应版本 [scala 2.11.11](https://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz)

假设解压到 /home/cc/scala-2.11.11

可在以下任意一个bash配置文件：
/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc

添加如下配置
```
SCALA_HOME=/home/cc/scala-2.11.11
PATH=$SCALA_HOME/bin:$PATH
export SCALA_HOME
export PATH
```
source对应修改的bash文件，将环境变量生效，如：

source /etc/profile

**设置环境变量**

### spark安装

下载spark编译好的版本 [ 2.2.0 Pre-built for Apache Hadoop 2.7 and later ](http://mirror.bit.edu.cn/apache/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz)

假设解压到/home/cc/spark-2.2.0-bin-hadoop2.7

**设置环境变量**

可在以下任意一个bash配置文件：
/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc

添加如下配置
```
SPARK_HOME=/home/cc/spark-2.2.0-bin-hadoop2.7
PATH=$SPARK_HOME/bin:$PATH
export SPARK_HOME
export PATH
```
source对应修改的bash文件，将环境变量生效，如：

source /etc/profile

可通过命令 `spark-shell` 进入spark命令行，运行scala测试：

```
val lines = sc.textFile("/tmp/hellospark")
lines.count()
```

如果hdfs已经启动，这里会从hdfs加载文件

以下我们继续配置spark的集群启动

**配置**

拷贝spark-env.sh.template，生成spark-env.sh配置文件，编辑spark-env.sh：

```
export SCALA_HOME=/home/cc/scala-2.11.11
export HADOOP_HOME=/home/cc/hadoop-2.8.2
export HADOOP_CONF_DIR=/home/cc/hadoop-2.8.2/etc/hadoop
#export SPARK_MASTER_IP=localhost
#export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8180
export SPARK_WORKER_MEMORY=1g
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
```

设置hadoop和scala的环境变量，如果曾经设置过，这里可以不用设置；设置hadoop配置文件目录；其余为可选设置，包括：

master ip，端口，webui的端口，限制work进程的cpu核心数，内存使用多大等。

编辑slaves.template，设置slave节点机器：

```
localhost
```

**集群启动**

执行`sbin/start-all.sh`, 启动spark集群。可以看到多了 Master 和 Worker 进程。也可以通过webui监控页面查看：

`http://localhost:8180/` #因为spark配置里设置了webui端口好为8180，不然默认是8080。监控页面里会显示以下重要信息：

```
URL：master url，提交spark作业时要用到这个url，提交作业时不可用localhost代替显示的host名称
REST URL:
workers信息
以及目前运行的作业和完成的作业信息
```

可以运行一个spark的sample：

```
cd /home/cc/spark-2.2.0-bin-hadoop2.7/bin
spark-submit --master spark://MacBook-Pro.local:7077  --class org.apache.spark.examples.SparkPi ../examples/jars/spark-examples_2.11-2.2.0.jar
```

### idea开发环境

在idea上安装scala插件，idea默认使用sbt进行编译和下载包依赖。

**新建项目**

创建sbt项目，scala版本选择 2.11.11，也可以在build.bat进行配置：

```
name := "spark-demo"

version := "0.1"

scalaVersion := "2.11.11"

val sparkVersion = "2.2.0"

libraryDependencies ++= Seq(
  "org.apache.spark" % "spark-core_2.11" % sparkVersion,
  "org.apache.spark" % "spark-sql_2.11" % sparkVersion,
  "org.apache.spark" % "spark-mllib_2.11" % sparkVersion,
  "org.apache.spark" % "spark-hive_2.11" % sparkVersion
)
```

更改配置后，refresh project，同maven操作方式一样。然后就会开始下载依赖包

这里说下sbt下载更新很慢，可以考虑网上说的方式，使用阿里云仓库，在~/.sbt下新建文件repositories，添加如下配置：
```
[repositories]
local
aliyun: http://maven.aliyun.com/nexus/content/groups/public/
central: http://repo1.maven.org/maven2/
```
先优先local库，再阿里云仓库，再maven仓库

**创建SparkPi例子**

创建SparkPi类，这里包名称为cc.examples，后面提交作业的时候，是要带上包名的。

```
package cc.examples

import scala.math.random

import org.apache.spark.sql.SparkSession

/** Computes an approximation to pi */
object SparkPi {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("Spark Pi")
      .getOrCreate()
    val slices = if (args.length > 0) args(0).toInt else 2
    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
    val count = spark.sparkContext.parallelize(1 until n, slices).map { i =>
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (x*x + y*y <= 1) 1 else 0
    }.reduce(_ + _)
    println("Pi is roughly " + 4.0 * count / (n - 1))
    spark.stop()
  }
}
```

**运行项目**

运行项目，通常会报错：`A master URL must be set in your configuration`

这时候需要给idea配置master url参数，master url可以有如下几种：

```
local 本地单线程
local[K] 本地多线程（指定K个内核）
local[*] 本地多线程（指定所有可用内核）
spark://HOST:PORT 连接到指定的 Spark standalone cluster master，需要指定端口。
mesos://HOST:PORT 连接到指定的 Mesos 集群，需要指定端口。
yarn-client客户端模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。
yarn-cluster集群模式 连接到 YARN 集群。需要配置 HADOOP_CONF_DIR。
```

本地单线程运行方式：

点击edit configuration，在 VM options 中输入 `-Dspark.master=local`，设置为本地单线程运行即可。

Spark standalone cluster master运行方式：

在 VM options 中输入 `-Dspark.master=spark://MacBook-Pro.local:7077`，但是运行会报错：JavaDeserializationStream$$anon$1.resolveClass，应该是那里依赖有问题，后面再解决。

先编译成jar提交给spark集群。

`spark-submit --master spark://MacBook-Pro.local:7077  --class cc.examples.SparkPi /home/cc/samples/spark-demo/out/artifacts/spark_demo_jar/spark-demo.jar
`

会报错：Invalid signature file digest for Manifest main attributes

一种解决方式为，将打好包的 jar 里 META-INF 的 RSA DSA SF 文件删掉：

`zip -d spark-demo.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF`
